# S02.04 MongoDB : Exploration approfondie

L'interrogation de données est importante.

Mais, si vous connaissez déjà SQL, il est facile, efficace et intelligent de donner le schéma de la base de données comme contexte de fond à un LLM (chatGPT, Claude, ...) et de convertir la requête de SQL à MongoDB.

Ou simplement demander au LLM d'écrire la requête pour vous. À ce stade, ce qui importe, c'est votre capacité à lire, comprendre et remettre en question les requêtes écrites par le LLM. Pas de les écrire de zéro.

Ce qui est plus important, c'est d'apprendre ce qui caractérise réellement MongoDB

- Choisir MongoDB plutôt que SQL
- Modèles de conception de schéma
- Fonctionnalités spécifiques de MongoDB
- Optimisation des requêtes
- Création d'index


## SQL vs NoSQL

C'est la première question à se poser au début de la conception de votre application.

Nous en avons parlé la dernière fois. Mais il est intéressant d'examiner cela plus en détail.

En bref, les bases de données basées sur des documents ont du sens

*   web scale
*   schéma de données flexible

> Au fait, il est bien connu que MongoDB est webscale : https://www.youtube.com/watch?v=b2F-DItXtZs

Vous avez dépassé le simple magasin de données sqlite ou basé sur JSON.

Vers quoi devriez-vous vous tourner ensuite ?

Un excellent article sur quand utiliser NoSQL : <https://medium.com/@sqlinsix/when-to-use-sql-or-nosql-b50d4a52c157> (disponible en pdf dans le repo github)

Extrait :

> Quelques questions lorsque je compare ces différents back-ends :

*   À quelle fréquence vais-je lire et écrire les données ?
*   À quelle fréquence la structure des données va-t-elle changer ?
*   Quelle quantité de données est inconnue ?
*   À quoi ressemble le produit final ?


### L'exemple du workout - les données sont flexibles

Imaginez une application de workout où vous suivez vos séances.

Il y a une série d'entités compatibles avec une bdd SQL :

- un `user` a plusieurs `sessions`,
- une `gym` a plusieurs `users`,
- un `user` a un `subscription`,
- les `gyms` ont plusieurs `subscriptions` et vice versa.

Mais il y a aussi une entité **workout** qui, par nature, peut varier  beaucoup.

- poids et répétitions : dead lifts, clean and jerks, ...
- sans poids : dead lifts, clean and jerks, ...
- course, rowing, natation, ...
- vélos, marches, sauts en parachute, ...
- power yoga, pilates, Zumba, ...

Un schéma de données fixe n'est pas adapté.

Autre extrait du même article :

> "_La complexité des jointures serait énorme par rapport à l'entraînement d'une personne, car il peut y avoir des dizaines de combinaisons. De plus, les entraînements tels que les séries dégressives où le poids change constamment et où une plage de répétitions maximale (ou minimale) doit être respectée seraient extrêmement difficiles à suivre avec un schéma fixe pour une base SQL._"

L'application des workouts sont un bon exemple où une structure de données à **schéma flexible** a vraiment du sens.

Il faut surtout réfléchir à la manière dont l'application consomme les données.

Par exemple si l'on souhaite enregistrer ou récupérer l'intégralité d'un entraînement pour un jour donné.

Dans une base SQL normalisée (une info existe dans une table unique) nous aurions besoin de faire un nombre de jointure important entre la table ed `session` et celles des différents type d'`exercices` et du workout du jour.

Avec une bdd NoSQL de type document (MongoDB), comme l'entité  workout est stockée dans son intégralité comme une seule information, dans un seul document, il n'y a plus besoin de ces jointures entre plusieurs tables.

Note : PostgreSQL dispose désormais de types de données JSON qui peuvent être utilisés pour un tel cas de données polymorphes.

### OLTP vs OLAP

Avez vous  besoin d'une base OLAP ou OLTP ?

Et dans quel cas une bdd de type MongoDB serra t elle plus adaptée que une bdd SQL ?

On considère 2 principaux types d'utilisation des bases de données : transactions ou tableaux de bord

#### OLTP

D'un coté, les systèmes **OLTP** (Online Transaction Processing) se concentrent sur la gestion d'un grand nombre de transactions courtes et atomiques en temps réel. Ils sont optimisés pour les opérations d'écriture et l'intégrité transactionnelle, couramment utilisés pour des applications telles que le commerce électronique, la banque et la gestion des stocks.

Dans les systèmes OLTP, on optimise au maximum la fiabilité des écritures, l'intégrité des données et la normalisation de la base.

Pensez OLAP ~= transactions.

#### OLAP

D'un autre coté, les systèmes **OLAP** (Online Analytical Processing) sont conçus pour interroger et analyser de grands volumes de données, impliquant souvent des jointures complexes, des agrégations et des transformations de données.

Ces systèmes sont _intensifs en lecture_ et nécessitent une récupération de données optimisée. La dénormalisation simplifie les requêtes et améliore les performances.

Pensez OLAP ~= tableaux de bord.

Donc, la grande question est la suivante: _doit-on utiliser NoSQL ou SQL pour OLAP ? pour OLTP ?_

- Pour les applications OLTP : Le coté document de MongoDB est parfait pour la gestion de nombreuses transactions petites et rapides, car chaque document contient toutes les données.

- Pour les applications OLAP : MongoDB présente certaines limitations qui le rendent moins idéal pour les requêtes analytiques complexes :

  - Le pipeline d'agrégation (utilisé pour les requetes complexes) est puissant, mais il peut devenir assez complexe pour des requêtes a visées analytiques, par essence  compliquées. Un  schéma SQL fixe et normalisé pourrait être plus simple.
  - En MongoDB, joindre de grandes quantités de données appartenant à différentes collections sera gourmand en mémoire et plus lent si' l'on compare avec des bases de données SQL.
  - Le moteur de stockage de MongoDB est optimisé pour les schémas d'accès aléatoires (typiques de l'OLTP) plutôt que pour les analyses séquentielles courantes dans les charges de travail analytiques.

## [EXERCICE] questionner un LLM

**Scénario** : si vous transférez la charge de travail des jointures et de l'agrégation au niveau de la collection (par le biais de tâches d'arrière-plan régulières par exemple), alors un magasin de documents tel que MongoDB devient-il compétitif pour les applications OLAP.

- Demander a un LLM (chatGPT, DeepSeek, Claude.ai, Qwen, Mistral, ) ce qu'il en pense.
- Comparer les reponses 2 a 2
- quels sont les arguments avancés

### Morale de l'histoire

Cette phrase met en lumière quand utiliser NoSQL

extrait de : https://softwareengineering.stackexchange.com/a/355964/440337

> _NOSQL est très largement conçu comme une couche de persistance pour une application._
> _L'accès est donc optimisé pour un seul objet avec des enfants._

en VO

> _NOSQL is very much designed as a persistence layer for an application_
> _So access is optimized for a single object with children._



Beaucoup de choses à décortiquer ici :

- **couche de persistance** fait référence au stockage et à la récupération de données afin qu'elles survivent au-delà de la durée d'exécution de l'application.

Les bases de données NoSQL (Document) sont construites spécifiquement pour répondre aux besoins de **stockage de données** des applications. Le mot important étant "Application". Les bases de données SQL sont conçues pour l'organisation et les relations de données, mais les besoins des applications étant secondaires.

Un **seul objet** fait référence à une entité de données principale (comme un profil d'utilisateur, un produit, une playlist ou un workout)

Les children ou **enfants** font référence aux données associées qui appartiennent ou sont étroitement associées à cet objet principal (comme les adresses d'un utilisateur, les avis sur un produit ou les exercices d'un workout)

Pourquoi est-ce optimisé pour un  _objet unique_ avec des _enfants_ ?

**unique** est un mot important ici :


Les bases de données NoSQL stockent physiquement l'objet principal et ses sous dépendants ensemble dans un **emplacement unique** sur le disque. Au lieu de diviser les données associées sur plusieurs tables comme dans les bases de données relationnelles.

La récupération de toutes les données associées nécessite moins d'opérations sur le disque et est moins gourmande en ressources et donc plus rapide.

Ce qui implique également un principe architectural clé : les bases de données NoSQL sont conçues autour des manières spécifiques dont les applications ont besoin d'accéder et de manipuler les données, plutôt que de maintenir des relations formelles de données .

Besoin de l'application > Organisation structurée des données

Cela les rend particulièrement efficaces lorsqu'une application a besoin de rapidement récupérer / mettre à jour  un objet entier avec toutes ses données associées et ceci en une seule opération.


[TODO] récapituler

### Baisse de Performance

Plusieurs facteurs techniques peuvent affecter les performances :

- **Indexation :** Tout d'abord, les bases de données NoSQL ne maintiennent pas le même type d'index sophistiqués que les bases de données relationnelles pour joindre les données réparties dans différentes collections. Le rôle des index dans MongoDB est d'aider à localiser des documents individuels plutot que d'optimiser les requêtes complexes sur plusieurs documents.

- **Accès disque :** DeuxiEnsuiteèmement, lorsque vous devez accéder à plusieurs objets qui ne sont pas physiquement stockés à proximité les uns des autres, la base de données doit effectuer plusieurs recherches aléatoires sur le disque.

Ces recherches aléatoires sont beaucoup plus lentes que les lectures séquentielles, car la tête de lecture du disque doit se déplacer physiquement vers différents emplacements. C'est pourquoi les opérations qui nécessitent l'analyse de nombreux documents (comme les requêtes analytiques) peuvent être plus lentes dans MongoDB que dans les bases de données relationnelles.

- **Gestion de la mémoire :** Enfin, la gestion de la mémoire de MongoDB est optimisée autour du concept d'ensembles de travail (working sets) :  les documents qui sont le plus fréquemment consultés. Sur de grands volumes de données qui dépassent la taille de la RAM, MongoDB doit constamment échanger des documents en dedans et en dehors de la mémoire avec  un impact probable sur les performances.

Sur le point 2, d'Accès disque : Le problème de données réparties un peu partout sur le disque physique à des emplacements aléatoires est vrai pour toutes les bases de données.

Mais PostgreSQL (par exemple) est beaucoup plus optimisé pour ce genre de tâches que ne l'est MongoDB.

Il existe des différences architecturales clés dans la manière dont les deux bases de données gèrent les opérations multi-enregistrements :

- Disposition du stockage : meilleure organisation du stockage et plus de contraintes dans PostgreSQL

  - PostgreSQL stocke les données dans des tables avec des schémas fixes, en utilisant un concept appelé "fichiers heap" où les enregistrements sont stockés dans des blocs/pages. Ces pages sont conçues pour un balayage séquentiel efficace.

  - MongoDB stocke chaque document indépendamment, potentiellement avec des tailles et des schémas variables. Cela peut conduire à plus de fragmentation et à un accès séquentiel moins efficace.

- Méthodes d'accès aux données : meilleure planification des requêtes

  - PostgreSQL a une planification de requêtes très sophistiquée  et donc très efficace qui choisi entre plusieurs méthodes d'accès :
    - Des analyses séquentielles (Sequential scans) qui lisent les pages efficacement dans l'ordre
    - Des analyses d'index (Index scans) qui peuvent utiliser plusieurs index simultanément
    - Des analyses bitmap (Bitmap scans) qui peuvent combiner les résultats de plusieurs type d'index
  - MongoDB s'appuie principalement sur des analyses d'index unique (single-index scans) ou des analyses de collection (collection scans), avec moins de facultées pour combiner les méthodes d'accès

- Opérations de jointure :
  - PostgreSQL dispose d'algorithmes spécialisés (jointures hachées, jointures par fusion, boucles imbriquées) - (hash joins, merge joins, nested loops) - qui sont optimisés pour combiner efficacement les données de plusieurs tables
  - MongoDB doit effectuer des recherches un document à la fois lors de la combinaison de données entre les collections, ce qui signifie souvent plus d'E/S aléatoires

- Gestion des buffers :
  - Le gestionnaire de buffer de PostgreSQL est conçu pour optimiser les schémas d'accès séquentiels et aléatoires
  - Le moteur de stockage WiredTiger de MongoDB est principalement optimisé pour les schémas d'accès aléatoires

Ainsi, bien que les deux bases de données doivent effectuer des opérations sur disque, PostgreSQL inclut des optimisations spécifiques pour gérer efficacement les opérations multi-enregistrements, en particulier lorsqu'il s'agit de jointures et d'analyses de grandes tables.

Ces optimisations sont moins présentes dans MongoDB car son architecture donne la priorité aux opérations sur un seul document.

### Conclusion sur SQL vs NoSQL - PostgreSQL vs MongoDB

**Impossible d'échapper à la complexité.**

La vitesse n'est pas le seul facteur à prendre en compte lors du choix d'une base de données.

Indépendamment de toute considération technique, la disponibilité et la productivité des ingénieurs ont plus d'impact sur les coûts que quelques millisecondes gagnées sur une requete.

Ce qui motive l'adoption de MongoDB par rapport à PostgreSQL est

- Comment l'application consomme les données
- le stockage et la récupération d'enregistrements uniques
- la complexité des données

## Performance

### Comparaison des performances PostgreSQL vs MongoDB

Lisez cet article <https://medium.com/@vosarat1995/postgres-vs-mongo-performance-comparison-for-semi-structured-data-9a5ec6486cf6>

L'auteur crée un grand jeux de données et teste les performances de MongoDB et de PostgreSQL.

```bash
| Test                                          | Mongo     | PostgreSQL |
| --------------------------------------------- | --------- | ---------- |
| Création d'index sur une base de données vide | 152.9 ms  | 402.0 ms   |
| Insertion par lot                             | 53.50 ms  | 219.15 ms  |
| Insertion un par un                           | 319.3 ms  | 641.9 ms   |
| Lecture de plusieurs enregistrements          | 21.83 ms  | 66.63 ms   |
| Création d'index sur 1000 rows                | 1.563 s   | 1.916 s    |
| Requête Complexe                              | 174.51 ms | 60.43 ms   |
```

MongoDB gagne la plupart du temps.

Mais PostgreSQL gagne au sprint final!

De plus : "_It is worth noting that the tests were on semi-structured data which is the realm of Mongo._"

### Explication de la planification des requêtes dans MongoDB

Sans entrer pour l'instant dans les détails de creation des index avec MongoDB analysons la planification des requêtes.

Pour `EXPLAIN` une requête:

- requête directe : `db.movies.find(<query predicate>).explain("executionStats")`
- pipelines d'agrégation : `db.movies.explain("executionStats").aggregate(<the_pipeline>)`

Notez les éléments suivants

- `totalDocsExamined`
- `nReturned`
- `executionTimeMillisEstimate` : estime le temps d'exécution cumulatif pour le pipeline jusqu'à l'étape en question incluse.
- `rejectedPlans`
- `stage` : COLLSCAN, IXSCAN

comparez : `db.movies.find({ "title": "The Perils of Pauline" }).explain("executionStats")`

Ensuite, créez un index `db.movies.createIndex({ title: 1 })`

à nouveau `db.movies.find({ "title": "The Perils of Pauline" }).explain("executionStats")`

Supprimez l'index `db.movies.dropIndex({title : 1})`

Expliquez un pipeline d'agrégation `db.movies.explain("executionStats").aggregate([ { $group: { _id: "$genres", averageRating: { $avg: "$imdb.rating" } } }] )`

### Comparaison de la planification des requêtes avec PostgreSQL

Considérez un scénario de requêtes similaires qui implique de joindre/relier des données et d'agréger des résultats.

**Requête** : _trouver tous les films des années 1990 avec leurs réalisateurs, regroupés par réalisateur avec les notes moyennes._

Tout d'abord, MongoDB :

```bash
db.movies.aggregate([
  { $match: {
      year: { $gte: 1990, $lt: 2000 }
  }},
  { $unwind: "$directors" },
  { $group: {
      _id: "$directors",
      avgRating: { $avg: "$imdb.rating" },
      movieCount: { $sum: 1 }
  }}
]).explain("executionStats")
```


Le plan est:

```json
{
  "stages": [
    {
      "$cursor": {
        "queryPlanner": {
          "plannerVersion": 1,
          "namespace": "sample_mflix.movies",
          "indexFilterSet": false,
          "parsedQuery": {
            "year": { "$gte": 1990, "$lt": 2000 }
          },
          "winningPlan": {
            "stage": "COLLSCAN",  // Analyse complète de la collection
            "filter": { "year": { "$gte": 1990, "$lt": 2000 } }
          },
          "rejectedPlans": []
        }
      }
    },
    { "$unwind": "$directors" },  // Opération gourmande en mémoire
    {
      "$group": {
        "_id": "$directors",
        "avgRating": { "$avg": "$imdb.rating" },
        "movieCount": { "$sum": 1 }
      }
    }
  ]
}
```

Maintenant, l'équivalent en PostgreSQL (en supposant un schéma normalisé) :

```sql
EXPLAIN ANALYZE
SELECT d.name as director,
  AVG(m.rating) as avg_rating,
  COUNT(*) as movie_count
FROM movies m
JOIN movie_directors md ON m.id = md.movie_id
JOIN directors d ON md.director_id = d.id
WHERE m.year >= 1990 AND m.year < 2000
GROUP BY d.name;
```

Ce qui donne :

```bash
QUERY PLAN
------------------------------------------------------------
HashAggregate  (cost=245.97..247.97 rows=200)
  Group Key: d.name
  ->  Hash Join  (cost=121.67..237.42 rows=1710)
        Hash Cond: (md.director_id = d.id)
        ->  Hash Join  (cost=66.50..164.42 rows=1710)
              Hash Cond: (md.movie_id = m.id)
              ->  Seq Scan on movie_directors md
              ->  Hash  (cost=58.00..58.00 rows=680)
                    ->  Index Scan using movies_year_idx on movies m
                          Index Cond: (year >= 1990 AND year < 2000)
        ->  Hash  (cost=40.50..40.50 rows=1173)
              ->  Seq Scan on directors d
```

Les principales différences dans la planification des requêtes sont :

1. Gestion des jointures :
   - PostgreSQL utilise des **jointures hachées** (**hash joins**) pour combiner efficacement les données de plusieurs tables
   - MongoDB doit `$unwind` le tableau imbriqué des réalisateurs, ce qui crée plusieurs documents en mémoire

2. Utilisation de l'index :
   - PostgreSQL peut utiliser **plusieurs index** simultanément et choisir différentes stratégies de jointure
   - MongoDB s'appuie généralement sur un seul index par étape de l'agrégation

3. Gestion de la mémoire :
   - Les jointures hachées de PostgreSQL créent des tables de hash en mémoire avec repli  disque si nécessaire
   - Les étapes `$unwind` et `$group` de MongoDB doivent conserver leurs données en mémoire

4. Ordre des opérations :
   - Le planificateur de requêtes de PostgreSQL peut **réorganiser les opérations** pour plus d'efficacité
   - MongoDB doit traiter les étapes du pipeline d'agrégation dans l'ordre

5. Utilisation des statistiques :
   - L'explication de PostgreSQL montre des estimations de coût détaillées basées sur les statistiques de la table
   - L'explication de MongoDB se concentre davantage sur le type d'opération et l'utilisation de l'index

Le plan PostgreSQL montre qu'il peut :

- Utiliser une analyse d'index pour le filtre d'année
- Créer des tables de hachage pour une jointure efficace
- Effectuer un regroupement avec agrégation hachée
- Estimer les coûts et le nombre de lignes à chaque étape

👽👽👽 Cette approche structurée des requêtes complexes est la raison pour laquelle PostgreSQL est souvent plus performant pour les charges de travail analytiques impliquant plusieurs tables/collections et agrégations.


