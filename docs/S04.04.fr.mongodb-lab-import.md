# S04.04 - Sprint

Dans ce sprint vous allez creer une nouvelle base de données MongoDB à partir d'un dataset de votre choix.

Vous pouvez travailler en solo ou par groupes de 2.

## Document de travail

Tout au long du projet vous documenterez vos travaux et votre experience.

Le document peut prendre la forme d'un **journal de bord** qui décrit votre expérience
ou un **tutorial** à destination de futurs étudiants.

Dans les 2 cas, le docucment doit inclure les élèments suivants:

- justifier le choix du dataset, son interet.
- décrire l'application qui va l'exploiter en pensant surtout a la facon dont l'applictaion va exploiter les données
- pourquoi le choix de mongodb plutot que une bdd SQL
- comment le choix du schema des données est en phase avec l'exploitation des données par l'application
- contexte matériel
- outils utilisés
Tout au long du projet,
- bugs, problèmes, et solutions trouvées
- performance

et pour finir

- une exploration du dataset avec des requètes en adéquation avec l'application
- analyse des données et de leurs performances


Le journal de bord / tutorial doit être un document google word.
Invitez moi en tant qu'editeur <alexis.perrier@gmail.com>

## Choix du dataset

Le choix du dataset est libre. Seule condition, il doit être assez grand, plusieurs Gb de données

- Kaggle datasets
- Nasa, ESA, ... datasets
- open data paris, data.gov.fr, ademe.fr etc
- huggingface

Si votre dataset est principaleent composé d'images, sons, vidéos, etc , il faudra trouver une solution de stockage cloud
pour ne pas stocker les images directemnt dans les documents MongoDB.

Le dataset peut comporter des textes longs (articles, livres, ...), des data IoT, etc ...

## Conception de l'application

Commencer par choisir le jeux de données et imaginer l'utilisation de ces données par une application.

Par exemple, données des 210k arbres de Paris:

- application de terrain d'enregistrement des dimensions des arbres
- application d'exploration des arbres de paris pour botanistes ou touristes
- application de gestion des arbres dédiée à la selection des types d'arbres et la geolocalisation

Quand vous avez une idée de votre application, pensez à la façon dont elle va consommer les données.
Quelles seront les operations et requetes les plus fréquentes dans l'application.

par exemple pour la premiere application de "terrain"

- trouver un arbre en fonction de sa geolocalisation et de son espèce
- enregistrer une nouvelle mesure des dimensions d'un arbre donnée
- visualizer l'évolution des arbres depuis leur plantation et comparer avec d'autres arbres de même espèce

etc...

## étapes ou sprint

Session 1: Choix du dataset et de l'application;
Session 2: Choix du schema et implementation de la validation
Session 3: Chargement des données et  transformation / nettoyage
Session 4: Test et optimisation


## Phase de Préparation

Préparez votre environnement de travail.

Notez les caractéristiques techniques de votre ordinateur qui influenceront votre stratégie d'importation.

Dans votre journal, notez par exemple:

- La quantité de RAM disponible
- L'espace disque libre
- Le type et la vitesse de votre processeur
- Votre système d'exploitation (Windows 11 ou MacOS)

## install mongodb

Installez ensuite MongoDB Community Edition.

[installing](https://www.mongodb.com/docs/manual/installation/)


Sur Windows 11, vous utiliserez l'installateur MSI disponible sur le site officiel.

Sur MacOS, vous pouvez utiliser Homebrew avec la commande appropriée.

Vérifiez que l'installation est réussie en démarrant le serveur MongoDB et en vous y connectant.

To check if `mongoimport` is already installed on your local, do `mongoimport --version`. If it returns the version you're fine, otherwise you must install it.

See this page to download [command line utilities](https://www.mongodb.com/try/download/database-tools). The list of utilities is [here](https://www.mongodb.com/docs/database-tools/). It includes `mongoimport`, `mongoexport`. see also `mongostat` and `mongotop` as diagnostic tools.


## Analyse et Conception du Schéma

Avant de commencer l'importation, examinez les fichiers CSV.

Dans votre documentation, décrivez :

- La structure des données présentes
- Les types de données pour chaque colonne
- Les relations potentielles entre les différentes données
- Les problèmes potentiels de qualité des données

À partir de cette analyse, concevez vos règles de validation de schéma.

Réfléchissez au niveau de validation que vous souhaitez mettre en place
et justifiez votre choix dans votre documentation.

## Planification de l'Importation

Calculez la taille totale de vos fichiers CSV et comparez-la à votre RAM disponible.

Dans votre carnet de bord, notez :

- La taille totale des données à importer
- La stratégie d'importation choisie (importation en bloc ou incrémentielle)
- Les raisons de votre choix
- Les risques potentiels et les solutions de contournement envisagées

## Stratégie d'Indexation

Identifiez les champs qui seront fréquemment interrogés dans vos requêtes.

Dans votre documentation, précisez :

- Les index que vous comptez créer
- Le moment choisi pour leur création (avant, pendant ou après l'importation)
- La justification de votre stratégie d'indexation

## Processus d'Importation

Commencez par tester votre approche avec un petit échantillon de données.

Surveillez les ressources système pendant l'importation et notez :

- La consommation de mémoire
- L'utilisation du processeur
- Le temps d'importation
- Les éventuels messages d'erreur ou avertissements

Si vous rencontrez des contraintes de mémoire, ajustez la taille des lots d'importation (batch size)
ou utilisez l'option `--numInsertionWorkers` de `mongoimport`.

## Vérification et Validation

Après l'importation, vérifiez l'intégrité de vos données.

Dans votre journal, incluez :

- Les requêtes utilisées pour vérifier les données
- Le nombre total d'enregistrements importés
- Les résultats des tests de validation du schéma
- Les performances des requêtes avec vos index

## Bilan et Réflexion

Votre journal doit / peut inclure

- Un résumé chronologique des étapes réalisées
- Les difficultés rencontrées et les solutions trouvées
- Les observations sur les performances système
- Une analyse critique de votre approche
- Des suggestions d'amélioration pour une future importation
- Les leçons apprises durant ce processus

N'oubliez pas que les erreurs et les difficultés font partie du processus d'apprentissage.



## Importer


We have several options to load the data from a JSON file. Either in mongosh with `insertMany()` or from the command line with `mongoimport`.

#### In mongosh with `insertMany()`

The following script

- loads the data from the `trees.json` file
- Insert data into the `trees` collection with `insertMany()`
- times the operation

```js
// load the JSON data
const fs = require("fs");
const dataPath = "./trees_1k.json"
const treesData = JSON.parse(fs.readFileSync(dataPath, "utf8"));

// Insert data into the desired collection
let startTime = new Date()
db.trees.insertMany(treesData);
let endTime = new Date()
print(`Operation took ${endTime - startTime} milliseconds`)
```

#### Using `mongoimport` command line

`mongoimport`  is usually the fastest option for large volumes.

By default `mongoimport` takes a ndjson file (one document per line ) as input. But you can also use a JSON file (an array of documents) if you add the flag `--jsonArray`.

The overall syntax for `mongoimport` follows:

```bash
mongoimport --uri="mongodb+srv://<username>:<password>@<cluster-url>" \
--db <database_name> \
--collection <collection_name> \
--file <path to ndjson file>
```

Here are other interesting, self explanatory, flags that may come in handy:

- `--mode=[insert|upsert|merge|delete]`
- `--stopOnError`
- `--drop` (drops the collection first )
- `--stopOnError`

In our context, here is a version of the command line, using the `MONGO_ATLAS_URI` environment variable and loading the JSON file `trees_1k.json` in the current folder.

```bash
time mongoimport --uri="${MONGO_ATLAS_URI}" \
--db treesdb  \
--collection trees \
--jsonArray \
--file ./trees_1k.json
```

which results in

```bash
2024-12-13T11:41:45.941+0100 connected to: mongodb+srv://[**REDACTED**]@skatai.w932a.mongodb.net/
2024-12-13T11:41:48.942+0100 [########################] treesdb.trees	558KB/558KB (100.0%)
2024-12-13T11:41:52.087+0100 [########################] treesdb.trees	558KB/558KB (100.0%)
2024-12-13T11:41:52.087+0100 1000 document(s) imported successfully. 0 document(s) failed to import.
mongoimport --uri="${MONGO_ATLAS_URI}" --db treesdb --collection trees  --fil  0.15s user 0.09s system 3% cpu 6.869 total
```

